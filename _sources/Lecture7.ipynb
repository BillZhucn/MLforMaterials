{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Mildred Dresselhaus\n",
    ":class: tip\n",
    "People said youâ€™re crazy... But if you think youâ€™re right, stick to it. And we were right.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe class=\"speakerdeck-iframe\" frameborder=\"0\" src=\"https://speakerdeck.com/player/31699a6d5e6c47f1be25e6dd16af566b\" title=\"Machine Learning for Materials (Lecture 7)\" allowfullscreen=\"true\" style=\"border: 0px; background-clip: padding-box; background-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 420;\" data-ratio=\"1.3333333333333333\"></iframe>\n",
    "\n",
    "[Lecture slides](https://speakerdeck.com/aronwalsh/mlformaterials-lecture7-build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¦¾ Crystal hardness revisited\n",
    "\n",
    "We first tackled the [bulk modulus](https://en.wikipedia.org/wiki/Bulk_modulus) of inorganic crystals in Lecture 3. However our model development was not thorough. \n",
    "\n",
    "Let's revisit this problem using some new tricks we have picked up. We will follow the same initial steps, making use of `matminer` (https://matminer.readthedocs.io) to access the materials dataset and featurise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of libraries\n",
    "!pip install matminer==\"0.9.0\" --quiet\n",
    "!pip install pymatgen==\"2023.09.25\" --quiet\n",
    "!pip install xgboost --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of modules\n",
    "import numpy as np  # Numerical operations\n",
    "from numpy import ComplexWarning  # Warning for complex numbers in NumPy\n",
    "from pymatgen.core import Structure  # Materials analysis for crystal structures\n",
    "from monty.serialization import loadfn  # Load serialised data\n",
    "import pandas as pd  # Data manipulation with DataFrames\n",
    "import matminer  # Materials informatics\n",
    "from matminer.datasets.dataset_retrieval import load_dataset  # Load materials datasets\n",
    "import warnings  # Warning control\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns  # Statistical visualisation\n",
    "import pprint  # Pretty print data structures\n",
    "plt.style.use('ggplot')  # Set Matplotlib style to 'ggplot'\n",
    "warnings.filterwarnings(\"ignore\", category=ComplexWarning)  # Ignore ComplexWarning\n",
    "\n",
    "# To make the model run faster\n",
    "teaching_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Colab error solution</summary>\n",
    "If running the import module cell fails with an \"AttributeError\", click `Runtime` -> `Restart Session` and then simply rerun the cell. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "The steps to load and featurise the data are introduced in Notebook 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matminer to load the dataset\n",
    "df = load_dataset('matbench_log_kvrh')\n",
    "print(f'The full dataset contains {df.shape[0]} entries. \\n')\n",
    "\n",
    "if teaching_mode:\n",
    "  # Store the original dataframe as a copy\n",
    "  full_dataset_df = df.copy()\n",
    "  # Create a subset of the original dataframe for demonstration purposes\n",
    "  df = df[:1500]\n",
    "  print(f'For teaching purposes we will only work with {df.shape[0]} entries from the dataframe to make the model training and testing faster. \\n')\n",
    "\n",
    "print('The dataframe is shown below:')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of values\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.hist(df['log10(K_VRH)'])\n",
    "ax.set_xlabel(r'$log_{10}K_{VRH}$ [$log_{10}GPa$]' )\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matminer to featurise the dataset\n",
    "from matminer.featurizers.composition.composite import ElementProperty\n",
    "from matminer.featurizers.structure.order import DensityFeatures\n",
    "\n",
    "# Add a composition column to df using the composition property of the Structure class and a lambda function\n",
    "df['composition'] = df.structure.apply(lambda x: x.composition )\n",
    "\n",
    "# Create the ElementProperty featuriser\n",
    "el_prop_featuriser = ElementProperty.from_preset(preset_name='magpie')\n",
    "\n",
    "# By default multiprocessing is enabled, however, this can slow performance, so we disable it\n",
    "el_prop_featuriser.set_n_jobs(1)\n",
    "\n",
    "# Featurise the dataframe using the ElementProperty featuriser\n",
    "df = el_prop_featuriser.featurize_dataframe(df, col_id='composition')\n",
    "\n",
    "# Add structure features\n",
    "density_featuriser = DensityFeatures()\n",
    "density_featuriser.set_n_jobs(1)\n",
    "df=density_featuriser.fit_featurize_dataframe(df, col_id='structure')\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the feature space a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature columns (excluding the first three)\n",
    "feature_columns = df.columns[3:]\n",
    "\n",
    "# Create a unique colour for each feature\n",
    "colors = [plt.cm.jet(i / float(len(feature_columns))) for i in range(len(feature_columns))]\n",
    "\n",
    "# Plot the distribution of feature values with different colours\n",
    "plt.figure(figsize=(5, 4))\n",
    "for i, column in enumerate(feature_columns):\n",
    "    df[column].plot(kind='hist', bins=20, alpha=0.5, color=colors[i], label=column)\n",
    "\n",
    "plt.title('Distribution of Feature Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dimensions have very different ranges, so we can standardise these. `MinMaxScaler` is a data scaling technique to transform numerical features within the range [0, 1]. It linearly scales data, preserving relationships between values, making it suitable for algorithms sensitive to feature magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaled_df = df.copy()\n",
    "\n",
    "# Step 1: Standardise the feature columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df[feature_columns] = scaler.fit_transform(scaled_df[feature_columns])\n",
    "\n",
    "# Step 2: Plot the standardised feature distributions\n",
    "plt.figure(figsize=(5, 4))\n",
    "for column in feature_columns:\n",
    "    scaled_df[column].plot(kind='hist', bins=20, alpha=0.5, label=column)\n",
    "\n",
    "plt.title('Standardised Feature Value Distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's prepare the data for model training. We need to split the dataset into the target variable `log10(K_VRH)` and the input features. For the input features, we must remove any non-numerical data to avoid getting errors later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features we want \n",
    "features_to_drop = ['structure','composition','log10(K_VRH)']\n",
    "feature_cols = [col for col in list(df.columns) if col not in features_to_drop]\n",
    "\n",
    "# Get an array of the features\n",
    "X = df[feature_cols].values\n",
    "scaled_X = scaled_df[feature_cols].values\n",
    "\n",
    "# Get an array of the target variable\n",
    "y = df['log10(K_VRH)'].values\n",
    "\n",
    "print(f'Shape of X: {X.shape}')\n",
    "print(f'Shape of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a supervised regression problem, so should choose a suitable model. We can start by rebuilding a random forest. Are you curious if the feature scaling has an effect? I am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest - original features\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestRegressor(n_estimators=100,criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X,y)\n",
    "\n",
    "# Wrap the lines of code for later sections\n",
    "def make_prediction_plot(X, y, model, label):\n",
    "    y_pred = model.predict(X)  # Calculate predictions here\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    ax.scatter(y, y_pred, c=y, cmap='viridis')\n",
    "    ax.plot(y, y, 'r-')\n",
    "    ax.set_xlabel(f'{label} True')\n",
    "    ax.set_ylabel(f'{label} Predicted')\n",
    "    plt.show()\n",
    "    return y_pred  # Return y_pred \n",
    "\n",
    "# Performance\n",
    "y_pred = make_prediction_plot(X, y, rf, 'log10(K_VRH)')  \n",
    "\n",
    "print(f'The training MAE = {metrics.mean_absolute_error(y,y_pred):.3f} log10GPa')\n",
    "print(f'The training RMSE = {np.sqrt(metrics.mean_squared_error(y,y_pred)):.3f} log10GPa')\n",
    "print(f'The training r^2 = {rf.score(X,y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest - scaled features\n",
    "\n",
    "# Define the model\n",
    "rf2 = RandomForestRegressor(n_estimators=100,criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf2.fit(scaled_X, y)\n",
    "\n",
    "# Performance\n",
    "y_pred = make_prediction_plot(scaled_X, y, rf2, 'log10(K_VRH)')  \n",
    "print(f'The training MAE = {metrics.mean_absolute_error(y, y_pred):.3f} log10GPa')\n",
    "print(f'The training RMSE = {np.sqrt(metrics.mean_squared_error(y, y_pred)):.3f} log10GPa')\n",
    "print(f'The training r^2 = {rf2.score(scaled_X, y):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Random Forest is not sensitive to feature scaling. Recall that this model works by averaging over multiple decision trees, and the decision boundaries are determined by feature thresholds, not their absolute values. \n",
    "\n",
    "We have time to try one more model. Let's go with the popular [XGBoost](https://xgboost.readthedocs.io). Like Random Forest, it is an ensemble learning method, but it uses a gradient-boosting framework and often achieves higher predictive accuracy by optimising for both bias and variance in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "\n",
    "# Define the model\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(scaled_X, y)\n",
    "\n",
    "# Performance\n",
    "y_pred = make_prediction_plot(scaled_X, y, xgb_model, 'log10(K_VRH)') \n",
    "print(f'The training MAE = {metrics.mean_absolute_error(y, y_pred):.3f} log10GPa')\n",
    "print(f'The training RMSE = {np.sqrt(metrics.mean_squared_error(y, y_pred)):.3f} log10GPa')\n",
    "print(f'The training r^2 = {xgb_model.score(scaled_X, y):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost seems to do a decent job, but we haven't performed proper training and testing yet. These models may be overfitting and unable to make predictions. On to the next stage!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and training\n",
    "\n",
    "### Train-test split\n",
    "\n",
    "We are ready to build a real model now. We will separate the training data from the unseen test set used to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the arrays\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "The library is \"sklearn\"!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation \n",
    "\n",
    "Using the 80% training set, we can train a model by making use of cross-validation in an attempt to avoid overfitting. Note that this step may take several minutes as 10 models are being trained (i.e. 5-fold cross-validation x 2 models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the models\n",
    "xgb_model = XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation for XGBoost\n",
    "xgb_cv_scores = -cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_rmse = np.sqrt(xgb_cv_scores)\n",
    "\n",
    "# Perform cross-validation for Random Forest\n",
    "rf_cv_scores = -cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_rmse = np.sqrt(rf_cv_scores)\n",
    "\n",
    "# Compare the results\n",
    "print(\"XGBoost Cross-Validation Results\")\n",
    "print(f\"  Mean RMSE: {xgb_rmse.mean():.3f}\")\n",
    "print(f\"  Standard Deviation of RMSE: {xgb_rmse.std():.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest Cross-Validation Results\")\n",
    "print(f\"  Mean RMSE: {rf_rmse.mean():.3f}\")\n",
    "print(f\"  Standard Deviation of RMSE: {rf_rmse.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamater optimisation\n",
    "\n",
    "XGBoost is narrowly in the lead! So far, we have not adjusted the models themselves. It is possible to improve performance by tuning the hyperparameters. Manually tuning would be laborious. We can use `GridSearchCV` to automate the search. \n",
    "\n",
    "Note that this step will be even more computationally expensive as we are performing cross-validation as a function of model hyperparameters for two separate models. You can see how computational cost quickly escalates and this is where powerful GPUs can become essential for machine learning! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning - this cell will take 2-3 minutes to run, even with a limited search space\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter Grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(XGBRegressor(random_state=42), xgb_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters for XGBoost\n",
    "best_xgb_params = xgb_grid_search.best_params_\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "\n",
    "# Hyperparameter Grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2, 4]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters for Random Forest\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best models\n",
    "xgb_cv_scores = -cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_rmse = np.sqrt(xgb_cv_scores)\n",
    "\n",
    "rf_cv_scores = -cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_rmse = np.sqrt(rf_cv_scores)\n",
    "\n",
    "# Compare the results of the best models\n",
    "print(\"Best XGBoost Model Hyperparameters:\", best_xgb_params)\n",
    "print(\"Best XGBoost Cross-Validation Results\")\n",
    "print(f\"  Mean RMSE: {xgb_rmse.mean():.3f}\")\n",
    "print(f\"  Standard Deviation of RMSE: {xgb_rmse.std():.3f}\")\n",
    "\n",
    "print(\"\\nBest Random Forest Model Hyperparameters:\", best_rf_params)\n",
    "print(\"Best Random Forest Cross-Validation Results\")\n",
    "print(f\"  Mean RMSE: {rf_rmse.mean():.3f}\")\n",
    "print(f\"  Standard Deviation of RMSE: {rf_rmse.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was it worth the effort? Well we got a small improvement in the RMSE for both models.\n",
    "\n",
    "### Model assessment\n",
    "\n",
    "Now that we have our best trained models, let's see how they perform on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Test the best XGBoost model\n",
    "xgb_test_preds = best_xgb_model.predict(X_test)\n",
    "xgb_test_rmse = np.sqrt(mean_squared_error(y_test, xgb_test_preds))\n",
    "\n",
    "# Test the best Random Forest model\n",
    "rf_test_preds = best_rf_model.predict(X_test)\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_preds))\n",
    "\n",
    "# Print test results\n",
    "print(\"XGBoost test results:\")\n",
    "print(f\"RMSE: {xgb_test_rmse:.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest test results:\")\n",
    "print(f\"RMSE: {rf_test_rmse:.3f}\")\n",
    "\n",
    "# Create a scatter plot with both models in different colors\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.scatter(y_test, xgb_test_preds, c='blue', label='XGBoost', alpha=0.5)\n",
    "plt.scatter(y_test, rf_test_preds, c='green', label='Random Forest', alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Test set performance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model speed\n",
    "\n",
    "The speed of a model may be important, e.g. a use case involving millions of predictions. Several factors can influence the computational performance, including the dataset size, model complexity, and hardware. We can perform a simple comparison of our two models using `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure the training time for XGBoost\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_training_time = time.time() - start_time\n",
    "\n",
    "# Measure the training time for Random Forest\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_training_time = time.time() - start_time\n",
    "\n",
    "# Measure the prediction time for XGBoost\n",
    "start_time = time.time()\n",
    "xgb_test_preds = xgb_model.predict(X_test)\n",
    "xgb_prediction_time = time.time() - start_time\n",
    "\n",
    "# Measure the prediction time for Random Forest\n",
    "start_time = time.time()\n",
    "rf_test_preds = rf_model.predict(X_test)\n",
    "rf_prediction_time = time.time() - start_time\n",
    "\n",
    "print(f\"XGBoost training time: {xgb_training_time:.3f} seconds\")\n",
    "print(f\"Random Forest training time: {rf_training_time:.3f} seconds\")\n",
    "print(f\"XGBoost prediction time: {xgb_prediction_time:.3f} seconds\")\n",
    "print(f\"Random Forest prediction time: {rf_prediction_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸš¨ Exercise 7: Best of the best\n",
    "\n",
    "\n",
    "```{admonition} Coding exercises\n",
    ":class: note\n",
    "The exercises are designed to apply what you have learned with room for creativity. It is fine to discuss solutions with your classmates, but the actual code should not be directly copied.\n",
    "\n",
    "The completed notebooks are to be submitted at the end of class, but you can revist later, experiment with the code, and follow the further reading suggestions.\n",
    "```\n",
    "\n",
    "\n",
    "### Your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your values\n",
    "Name = \"No Name\" # Replace with your name\n",
    "CID = 123446 # Replace with your College ID (as a numeric value with no leading 0s)\n",
    "\n",
    "# Set a random seed using the CID value\n",
    "CID = int(CID)\n",
    "np.random.seed(CID)\n",
    "\n",
    "# Print the message\n",
    "print(\"This is the work of \" + Name + \" [CID: \" + str(CID) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tasks\n",
    "\n",
    "Selecting the most appropriate ML model for a given purpose is important for achieving predictive performance. Your job is to assess additional models for the hardness regression task: \n",
    "\n",
    "1.  Train a [k-nearest neighbour regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) model and assess its performance on the test set compared to XGBoost. Some starter code is given below.\n",
    "\n",
    "*Self-study (optional)*  \n",
    "\n",
    "2.  Extend to a [support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) model or other relevant regression models that you find.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# k-Nearest Neighbors Regression\n",
    "knn_model = KNeighborsRegressor(n_neighbors=6)  # You can also try different n\n",
    "knn_model.fit(X_train, y_train)  # Train the KNN model on the training set\n",
    "\n",
    "# Support Vector Regression\n",
    "svr_model = SVR(kernel='linear') # You could also try 'rbf' or 'poly'\n",
    "svr_model.fit(X_train, y_train)  # Train the SVR model on the training set\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary> Task hint </summary>\n",
    "You can perform cross-validation following the same procedure as the random forest model in the notebook\n",
    "</details>\n",
    "\n",
    "```{admonition} Submission\n",
    ":class: note\n",
    "When your notebook is complete, click on the download icon on the top right, select `.pdf`, save the file and upload it to MyDepartment. If you are using Google Colab, you have to print to pdf.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŠ Dive deeper\n",
    "\n",
    "* _Level 1:_ Tackle Chapter 14 on Tree-Based Learners in [Machine Learning Refined](https://github.com/jermwatt/machine_learning_refined#what-is-new-in-the-second-edition). \n",
    "\n",
    "* _Level 2:_ Explore the XGBoost [tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/model.html), e.g. predicting multiple properties with multi-output regression. \n",
    "\n",
    "* _Level 3:_ Find the best model (subject to time constraints) with [Automatminer](https://hackingmaterials.lbl.gov/automatminer) based on [TPOT](https://epistasislab.github.io/tpot)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
