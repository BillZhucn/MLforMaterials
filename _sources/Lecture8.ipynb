{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recent Advances in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Geoffrey Hinton \n",
    ":class: tip\n",
    "Itâ€™s quite conceivable that humanity is just a passing phase in the evolution of intelligence.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe class=\"speakerdeck-iframe\" frameborder=\"0\" src=\"https://speakerdeck.com/player/e7872bd00d8348bcbf8f02720d5f36b6\" title=\"Machine Learning for Materials (Lecture 8)\" allowfullscreen=\"true\" style=\"border: 0px; background-clip: padding-box; background-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 420;\" data-ratio=\"1.3333333333333333\"></iframe>\n",
    "\n",
    "[Lecture slides](https://speakerdeck.com/aronwalsh/mlformaterials-lecture8-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– x ðŸ§ª Closed-loop optimisation \n",
    "\n",
    "The combination of automation and optimisation is powerful. Closed-loop optimisation is of growing importance in materials research for many reasons, including:\n",
    "\n",
    "1. **Efficiency:** Efficient allocation of resources, both in terms of time and materials. By continuously updating experimental parameters based on real-time feedback, we can reduce the number of trials needed to reach optimal outcomes. \n",
    "\n",
    "2. **Adapt to changing conditions:** Adaptive decision-making, ensuring that experiments remain effective even when external factors fluctuate. This adaptability is highly valuable for complex systems where traditional trial-and-error approaches are prone to fail.\n",
    "\n",
    "3. **Exploration of large parameter spaces:** Many materials science problems involve high-dimensional parameter spaces where exhaustive exploration is impractical. Techniques such as Bayesian optimisation can efficiently sample and search these spaces to identify optimal configurations and make discoveries.\n",
    "\n",
    "4. **Data-driven insights:** Generation of valuable data from ongoing experiments. This data can be analysed to gain a deeper understanding of the underlying processes and relationships, facilitating scientific discoveries and supporting future efforts.\n",
    "\n",
    "Today we will make use of the [scikit-optimise](https://scikit-optimize.github.io) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of libraries\n",
    "!pip install scikit-optimize --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of modules\n",
    "import numpy as np  # Numerical operations\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from scipy.stats import norm  # Statistical functions\n",
    "from skopt import gp_minimize, dummy_minimize  # Bayesian optimisation\n",
    "from skopt.utils import create_result  # Utility functions for skopt\n",
    "from sklearn.metrics import r2_score  # R-squared metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian optimisation (BO)\n",
    "\n",
    "BO is a powerful technique for optimising complex and expensive-to-evaluate functions. It combines probabilistic modeling and decision theory to efficiently search for the optimal set of parameters. In materials research, relevant paramaters could be related to chemical composition, sample thickness, processing conditions, etc.\n",
    "\n",
    "BO seeks to find the global minimum (or maximum) of an objective function, $O(x)$, where $x$ represents a set of parameters or design variables. The primary challenge is that evaluating $O(x)$ can be costly and time-consuming, making traditional grid search or random search methods impractical in high dimensions.\n",
    "\n",
    "The central idea is to build a surrogate model, typically a Gaussian Process (GP), that approximates the true objective function. This surrogate model captures both the mean $\\mu(x)$ and uncertainty $\\sigma(x)$ associated with $O(x)$. The GP is defined as:\n",
    "\n",
    "$$\n",
    "O(x) \\sim \\text{GP}(\\mu(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "where $k(x, x')$ is a kernel function that quantifies the similarity between two input points $x$ and $x'$.\n",
    "\n",
    "The surrogate model guides the optimisation process by providing a probabilistic representation of $O(x)$. It helps to balance the exploration-exploitation trade-off by suggesting the next set of paramaters to try. This decision is made using an acquisition function $\\alpha(x)$, which trades off between exploring uncertain regions and exploiting promising areas:\n",
    "\n",
    "$$\n",
    "x_{\\text{next}} = \\arg \\max_x \\alpha(x)\n",
    "$$\n",
    "\n",
    "Common acquisition functions include Probability of Improvement (PI), Expected Improvement (EI), and Upper Confidence Bound (UCB). Each of these functions aims to maximise the expected gain in performance over the current best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BO model\n",
    "\n",
    "### Step 1. Target function\n",
    "\n",
    "We can start by generating a simple sine-like target function with added noise to keep things interesting. This acts as our \"virtual experiment\", i.e. we can call the function to obtain an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the target function\n",
    "def target_function(x):\n",
    "    x = np.atleast_1d(x) # Ensure x is an array\n",
    "    return np.sin(x[0]) + 0.1 * x[0] + 0.5 * np.random.randn()\n",
    "\n",
    "# Generate data for visualisation\n",
    "x_values = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "y_values = np.vectorize(target_function)(x_values)\n",
    "\n",
    "# Plot the target function with semitransparent points\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly sample the target function and fit a simple polynomial function to get a feeling for how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample points from the target function\n",
    "num_initial_points = \n",
    "initial_points = np.random.uniform(-5, 5, num_initial_points)\n",
    "initial_values = np.vectorize(target_function)(initial_points)\n",
    "\n",
    "# Plot the sample points\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.scatter(initial_points, initial_values, color='blue', marker='o', label='Initial Samples')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "Try `num_initial_points = 10`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a polynomial fit\n",
    "degree =   # Adjust the degree of the polynomial fit\n",
    "coefficients = np.polyfit(initial_points, initial_values, degree)\n",
    "poly_fit = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate R^2\n",
    "y_pred = poly_fit(initial_points)\n",
    "r_squared = r2_score(initial_values, y_pred)\n",
    "\n",
    "# Plot the sample points and polynomial fit\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.scatter(initial_points, initial_values, color='blue', marker='o', label='Initial Samples')\n",
    "plt.plot(x_values, poly_fit(x_values), 'g--', label=f'Polynomial Fit (degree {degree})\\n$R^2 = {r_squared:.4f}$')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "Adjust the degree of the polynomial to see how good the fit is. Start with `degree = 2`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Gaussian Process\n",
    "\n",
    "Now we can move to Bayesian Optimisation with a Gaussian Process model. The optimisation progress is visualised by plotting the target function, optimisation steps, and a colourbar indicating the step number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning, this may take a minute to run. ML makes computers work hard!\n",
    "\n",
    "# Optimise the target function using Bayesian Optimisation\n",
    "result = gp_minimize(target_function, [(-5.0, 5.0)], n_calls=50, random_state=42)\n",
    "\n",
    "# Perform random sampling for comparison\n",
    "random_result = dummy_minimize(target_function, [(-5.0, 5.0)], n_calls=50, random_state=42)\n",
    "\n",
    "# Plot the Gaussian Process model after optimisation\n",
    "x_gp = np.array(result.x_iters).reshape(-1, 1)\n",
    "y_gp = result.func_vals\n",
    "\n",
    "# Plot the target function\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target function')\n",
    "\n",
    "# Plot the optimisation steps with a colormap\n",
    "plt.scatter(x_gp, y_gp, c=range(len(x_gp)), cmap='viridis', marker='o', label='Step number')\n",
    "\n",
    "# Add colorbar to indicate the progress\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Step number')\n",
    "\n",
    "plt.title('Bayesian Optimisation: Gaussian Process Model')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plot_gaussian_process` from scikit-optimize to visualise the confidence intervals. `n_samples` determines the number of samples to draw from the Gaussian Process for the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_gaussian_process as plot_gp\n",
    "\n",
    "# Plot the Gaussian Process model with confidence intervals\n",
    "plt.figure(figsize=(5, 4))\n",
    "plot_gp(result, n_samples=10, objective_ylim=(-5, 5), show_title=False)\n",
    "\n",
    "# Add the target function for reference\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target function')\n",
    "\n",
    "plt.title('Confidence Intervals')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should always have a benchmark to compare our model to. This block extracts the best results from BO and random sampling, then compares and visualises their performance over optimisation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cumulative minimum values\n",
    "bo_min_values = np.minimum.accumulate(result.func_vals)\n",
    "random_min_values = np.minimum.accumulate(random_result.func_vals)\n",
    "\n",
    "# Plot the cumulative minimum values vs steps for both methods\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(range(1, len(bo_min_values) + 1), bo_min_values, 'o-', label='Bayesian Optimisation')\n",
    "plt.plot(range(1, len(random_min_values) + 1), random_min_values, 'x-', label='Random Sampling')\n",
    "\n",
    "plt.title('Does BO Beat Random Sampling?')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Minimum Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ðŸš¨ Exercise 8: Closed-loop optimisation\n",
    "\n",
    "```{admonition} Coding exercises\n",
    ":class: note\n",
    "The exercises are designed to apply what you have learned with room for creativity. It is fine to discuss solutions with your classmates, but the actual code should not be directly copied.\n",
    "\n",
    "The completed notebooks are to be submitted at the end of class, but you can revisit later, experiment with the code, and follow the further reading suggestions.\n",
    "```\n",
    "\n",
    "### Your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your values\n",
    "Name = \"No Name\" # Replace with your name\n",
    "CID = 123446 # Replace with your College ID (as a numeric value with no leading 0s)\n",
    "\n",
    "# Set a random seed using the CID value\n",
    "CID = int(CID)\n",
    "np.random.seed(CID)\n",
    "\n",
    "# Print the message\n",
    "print(\"This is the work of \" + Name + \" [CID: \" + str(CID) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Imagine, the Department of Materials has invested in a new automated thin-film deposition system. The machine has two dials that provide a 2D parameter space for materials processing. We can define a (hypothetical) target loss function for optimising the transition temperature of our candidate thin-film superconductors as:\n",
    "\n",
    "```python\n",
    "# Target function for materials processing with x and y \"dials\"\n",
    "def supermat(inputs):\n",
    "    x, y = inputs\n",
    "    a = 1, b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6, s = 10, t = 1 / (8 * np.pi)\n",
    "\n",
    "    term1 = a * (y - b * x**2 + c * x - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x)\n",
    "    term3 = s\n",
    "\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Example usage:\n",
    "dials = [2.0, 3.0]\n",
    "result = supermat(dials)\n",
    "print(f\"Experiment by setting dials to ({dials[0]}, {dials[1]}): {result}\")\n",
    "```\n",
    "\n",
    "1. Perform Bayesian optimisation to find the minimum value of `supermat` using a Gaussian Process surrogate model (e.g. `gp_minimize`). The range of each dial is (0.0, 5.0), i.e.\n",
    "\n",
    "```python\n",
    "space = [(0.0, 5.0), (0.0, 5.0)]  # Range of x and y values\n",
    "\n",
    "```\n",
    "\n",
    "State the values of the optimal (x,y) paramaters and the global minumum value of the loss function. \n",
    "\n",
    "*Self-study (optional)*  \n",
    "\n",
    "2. Plot the results in the form of cumulative minimum vs iteration or a 2D contour plot, which indicate the global minimum value. \n",
    "\n",
    "3. Compare the performance of BO against random sampling for this case through a plot of the best value vs step number, and a direct comparison of the final solutions.\n",
    "\n",
    "<details>\n",
    "<summary> Task hint </summary>\n",
    "Remember to first define the target function and then call it using gp_minimize()\n",
    "</details>\n",
    "\n",
    "```{admonition} Submission\n",
    ":class: note\n",
    "When your notebook is complete, click on the download icon on the top right, select `.pdf`, save the file and upload it to MyDepartment. If you are using Google Colab, you have to print to pdf.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment block \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment block \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŠ Dive deeper\n",
    "\n",
    "* _Level 1:_ Read a perspective on [Bayesian optimisation for chemical problems](https://chemrxiv.org/engage/chemrxiv/article-details/656dfe74cf8b3c3cd7c611a5) by your teaching assistant Yifan, which includes links to tool and packages under development.\n",
    "\n",
    "* _Level 2:_ Interact with the self-driving laboratory demo by [Sterling Baird](https://github.com/sparks-baird/self-driving-lab-demo).\n",
    "  \n",
    "* _Level 3:_ Explore the limits of fine-tuned large-language models such as [Perplexity](https://www.perplexity.ai) and [ChemLLM](https://chemllm.org)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
